{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Person of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be using DreamAI to train a model. This is an example of what we can do by combining two general purpose models trained using DreamAI. We will use an object detection model and a face detection model to detect and blur out people who are not the person of interest (PoI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Make sure to change this path to your folder with DreamAI\n",
    "\n",
    "sys.path.insert(0, '/home/farhan/hamza/dreamai/') # Folder with DreamAI\n",
    "\n",
    "# Things below are all included in the dreamai folder\n",
    "\n",
    "import utils\n",
    "import obj_utils\n",
    "from dai_imports import*\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the face detection and the object detection models\n",
    "\n",
    "face_net = data_processing.load_obj('best_face_net.pkl')\n",
    "person_net = data_processing.load_obj('best_obj_net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate the intersection over unio (IoU) of two rectangles\n",
    "# Source: ronny rest (can't seem to find the link to their website)\n",
    "\n",
    "def get_iou(a, b, epsilon=1e-5):\n",
    "    \"\"\" Given two boxes `a` and `b` defined as a list of four numbers:\n",
    "            [x1,y1,x2,y2]\n",
    "        where:\n",
    "            x1,y1 represent the upper left corner\n",
    "            x2,y2 represent the lower right corner\n",
    "        It returns the Intersect of Union score for these two boxes.\n",
    "\n",
    "    Args:\n",
    "        a:          (list of 4 numbers) [x1,y1,x2,y2]\n",
    "        b:          (list of 4 numbers) [x1,y1,x2,y2]\n",
    "        epsilon:    (float) Small value to prevent division by zero\n",
    "\n",
    "    Returns:\n",
    "        (float) The Intersect of Union score.\n",
    "    \"\"\"\n",
    "    # COORDINATES OF THE INTERSECTION BOX\n",
    "    x1 = max(a[0], b[0])\n",
    "    y1 = max(a[1], b[1])\n",
    "    x2 = min(a[2], b[2])\n",
    "    y2 = min(a[3], b[3])\n",
    "\n",
    "    # AREA OF OVERLAP - Area where the boxes intersect\n",
    "    width = (x2 - x1)\n",
    "    height = (y2 - y1)\n",
    "    # handle case where there is NO overlap\n",
    "    if (width<0) or (height <0):\n",
    "        return 0.0\n",
    "    area_overlap = width * height\n",
    "\n",
    "    # COMBINED AREA\n",
    "    area_a = (a[2] - a[0]) * (a[3] - a[1])\n",
    "    area_b = (b[2] - b[0]) * (b[3] - b[1])\n",
    "    area_combined = area_a + area_b - area_overlap\n",
    "\n",
    "    # RATIO OF AREA OF OVERLAP OVER COMBINED AREA\n",
    "    iou = area_overlap / (area_combined+epsilon)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to expand a bounding box by some margin\n",
    "\n",
    "def expand_rect(left,top,right,bottom,H,W, margin = 15):\n",
    "    if top >= margin:\n",
    "        top -= margin\n",
    "    if left >= margin:\n",
    "        left -= margin\n",
    "    if bottom <= H-margin:\n",
    "        bottom += margin\n",
    "    if right <= W-margin:\n",
    "        right += margin\n",
    "    return left,top,right,bottom\n",
    "\n",
    "def dai_poi(video_path, face_net, person_net, detection_conf = 0.3, nms_overlap = 0.4,\n",
    "                   rec_tolerance = 0.5, iou_thresh = 0.3, frame_size = (256,256), output = 'poi_out.mp4'):\n",
    "    writer = None\n",
    "    \n",
    "    # initialize the video stream, then allow the camera sensor to warm up\n",
    "    vs = cv2.VideoCapture(video_path)\n",
    "    time.sleep(2.0)\n",
    "    frame_number = 0\n",
    "    poi = None\n",
    "    selected = False\n",
    "    gone = False\n",
    "    all_blur = True\n",
    "    faces_found = False\n",
    "    person_found = False\n",
    "    tracker = None\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # Grab a frame from the video stream\n",
    "        (grabbed, frame) = vs.read()\n",
    "        if grabbed:\n",
    "            frame_number+=1\n",
    "            old_H,old_W = frame.shape[:2]\n",
    "        # If the frame was not grabbed, then we have reached the end of the video\n",
    "        if not grabbed:\n",
    "            break\n",
    "            \n",
    "# Our model was trained on imgages of size 'frame_size' and in object detection, we can't run our model on\n",
    "# images of a different size, so we will resize the frame just for predictions\n",
    "        new_frame = cv2.resize(frame,frame_size)\n",
    "        (H, W) = new_frame.shape[:2]\n",
    "        height_scale = old_H/H\n",
    "        width_scale = old_W/W\n",
    "        \n",
    "        # Display the current info on the frame\n",
    "        \n",
    "        info = [\n",
    "                (\"Selected\", selected),\n",
    "                (\"All Blur\", all_blur)\n",
    "            ]\n",
    "        for (i, (k, v)) in enumerate(info):\n",
    "            text = \"{}: {}\".format(k, v)\n",
    "            cv2.putText(frame, text, (10, old_H - ((i * 20) + 20)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "        \n",
    "        # If PoI has been selected, update the tracker and draw the bounding box on the frame\n",
    "        \n",
    "        if selected:           \n",
    "            (success, box) = tracker.update(frame)\n",
    "            box = [int(box[0]),int(box[1]),int(box[2]+box[0]),int(box[1]+box[3])]\n",
    "\n",
    "            # Check to see if the tracking was a success\n",
    "            \n",
    "            if success:\n",
    "                (x1, y1, x2, y2) = box\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2),(0, 0, 255), 2)\n",
    "                cv2.rectangle(frame, (x1, y2 + 25), (x2, y2), (0, 0, 255), cv2.FILLED)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                cv2.putText(frame, 'POI', (x1 + 6, y2 + 19), font, 0.5, (255, 255, 255), 1)\n",
    "        \n",
    "        # 'all_blur' means that we should blur out every face whether it's the PoI or not\n",
    "        \n",
    "        if selected or all_blur:\n",
    "\n",
    "            # Take the resized 'new_frame' and turn it into a PyTorch batch to be fed to the model\n",
    "            \n",
    "            img_batch = utils.get_test_input(imgs=[new_frame],size=frame_size)\n",
    "            \n",
    "            # Predict. This will output the bounding boxes and the labels of all faces detected\n",
    "            \n",
    "            face_locations = face_net.predict_objects(img_batch,score_thresh=detection_conf,\n",
    "                                                 nms_overlap=nms_overlap)[0]\n",
    "            \n",
    "            faces_found = len(face_locations) > 0\n",
    "            if faces_found:\n",
    "                \n",
    "                # Since our 'new_frame' was the resized frame,\n",
    "                # we must scale the predicted bounding boxes back to the original frame size\n",
    "                \n",
    "                face_locations = [[int(np.ceil(f[0]*width_scale)),int(np.ceil(f[1]*height_scale)),\n",
    "                               int(np.ceil(f[2]*width_scale)),int(np.ceil(f[3]*height_scale))] \n",
    "                               for f in face_locations]\n",
    "                for (left, top, right, bottom) in face_locations:\n",
    "                    \n",
    "                    # If all_blur, blur the face, elif there's a PoI, blur every face but the PoI\n",
    "                    \n",
    "                    if all_blur:\n",
    "                        left,top,right,bottom = expand_rect(left,top,right,bottom,old_H,old_W,margin=40)\n",
    "                        sub_face = frame[top:bottom, left:right]\n",
    "                        sub_face = cv2.GaussianBlur(sub_face,(35,35), 100)\n",
    "                        try:\n",
    "                            frame[top:top+sub_face.shape[0], left:left+sub_face.shape[1]] = sub_face\n",
    "                        except:\n",
    "                            pass    \n",
    "                    elif success:\n",
    "                        iou = get_iou(box,(left, top, right, bottom))    \n",
    "                        if (iou < iou_thresh):\n",
    "                            left,top,right,bottom = expand_rect(left,top,right,bottom,old_H,old_W,margin=40)\n",
    "                            try:\n",
    "                                sub_face = frame[top:bottom, left:right]\n",
    "                                sub_face = cv2.GaussianBlur(sub_face,(35,35), 100)\n",
    "                                frame[top:top+sub_face.shape[0], left:left+sub_face.shape[1]] = sub_face\n",
    "                            except:\n",
    "                                pass\n",
    "                            \n",
    "            # Our face detector might miss some faces because of obstruction or a weird angle, so we will use\n",
    "            # our object detector to detect a person, and the blur out the top half of that person as well\n",
    "            # Both of these combined will guarantee that no face is missed\n",
    "\n",
    "            object_net_pred = person_net.predict_objects(img_batch,score_thresh=detection_conf,\n",
    "                                                 nms_overlap=nms_overlap)\n",
    "            object_found = len(object_net_pred[0]) > 0\n",
    "            if object_found:\n",
    "                person_locations = np.array(object_net_pred[0])[np.array(object_net_pred[1]) == 'person']\n",
    "                \n",
    "                if len(person_locations) > 0:\n",
    "                \n",
    "                    person_locations = [[int(np.ceil(f[0]*width_scale)),int(np.ceil(f[1]*height_scale)),\n",
    "                                       int(np.ceil(f[2]*width_scale)),int(np.ceil(f[3]*height_scale))] \n",
    "                                       for f in person_locations]\n",
    "                    for (left, top, right, bottom) in person_locations:\n",
    "\n",
    "                        half_coords = (left,top,right,((bottom-top)//2)+top)\n",
    "                        if all_blur:\n",
    "                            left,top,right,bottom = half_coords\n",
    "                            try:\n",
    "                                sub_face = frame[top:bottom, left:right]\n",
    "                                sub_face = cv2.GaussianBlur(sub_face,(35,35), 100)\n",
    "                                frame[top:top+sub_face.shape[0], left:left+sub_face.shape[1]] = sub_face\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif success:\n",
    "                            iou = get_iou(box,(left, top, right, bottom))\n",
    "                            left,top,right,bottom = half_coords\n",
    "                            if (iou < iou_thresh):\n",
    "                                sub_face = frame[top:bottom, left:right]\n",
    "                                sub_face = cv2.GaussianBlur(sub_face,(35,35), 100)\n",
    "                                try:\n",
    "                                    frame[top:top+sub_face.shape[0], left:left+sub_face.shape[1]] = sub_face\n",
    "                                except:\n",
    "                                    pass        \n",
    "                       \n",
    "        # Show the output frame\n",
    "\n",
    "        cv2.imshow(\"POI Video\", frame)\n",
    "        \n",
    "        # Write the frame to the output file\n",
    "        \n",
    "        if writer is None:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"DIVX\")\n",
    "            writer = cv2.VideoWriter(output, fourcc, 24,\n",
    "                (frame.shape[1], frame.shape[0]), True)\n",
    "        \n",
    "        writer.write(frame)\n",
    "        \n",
    "        # Slight delay if no PoI selected just to make it easier to select\n",
    "        \n",
    "        if not selected:\n",
    "            time.sleep(0.09)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # If 's' is pressed, video pauses and PoI can be selected. Press 'enter' or 'space' after selecting\n",
    "        \n",
    "        if key == ord(\"s\"):\n",
    "            \n",
    "            selected = True\n",
    "            all_blur = False\n",
    "            poi = cv2.selectROI(\"POI Video\", frame, fromCenter=False,\n",
    "                showCrosshair=True)\n",
    "\n",
    "            # Start OpenCV object tracker using the selected PoI\n",
    "            tracker = cv2.TrackerCSRT_create()\n",
    "            tracker.init(frame, poi)\n",
    "            \n",
    "        # If 'd' is pressed, toggle all_blur\n",
    "        \n",
    "        elif key == ord(\"d\"):\n",
    "            all_blur = not all_blur\n",
    "            \n",
    "        # If 'w' is pressed, remove the current PoI and set all_blur to True    \n",
    "            \n",
    "        elif key == ord(\"w\"):\n",
    "            selected = False\n",
    "            all_blur = True\n",
    "        \n",
    "        # If 'q' is pressed, quit\n",
    "            \n",
    "        elif key == ord(\"q\"):\n",
    "            break    \n",
    "            \n",
    "    writer.release()\n",
    "    vs.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'dai_poi' takes a video path and two models, a face detector and an object detector, and some additional paramaters.\n",
    "When run, it starts displaying the video in a window, there is a slight delay between frames so that it is easier to mark the region of interest (PoI).\n",
    "\n",
    "By default, every face is blurred out. Presssing 'd' will toggle 'all_blur' and then noone will be blurred.\n",
    "\n",
    "When the user presses 's', the video pauses and then they can draw a box on the PoI, then when they press 'space' or 'enter', the video resumes with the PoI now being displayed and tracked and all the other faces being blured.\n",
    "\n",
    "Pressing 'w' will reset the PoI and turn on 'all_blur'. Until the user presses 's' again, there will be no PoI.\n",
    "\n",
    "Pressing 'q' will quit the video and the final video with the bounding boxes being displayed will be saved in the 'output' file.\n",
    "\n",
    "You can see the output generated video called 'mall_poi_github.mp4' as well as the screen recording of the demo called 'poi_usage_demo.mp4'\n",
    "\n",
    "If you want to try it yourself, just run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'videos/mall.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dai_poi(video_path,face_net,person_net,detection_conf=0.1,nms_overlap=0.3,\n",
    "                        rec_tolerance=0.56,iou_thresh=0.01,\n",
    "                        output='mall_poi.mp4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
